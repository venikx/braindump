#+TITLE: Algorithms

* Computational Complexity
- Big O Notation $O$ :: worst-case scenario (upper bound)
- Big \Omega (Omega) Notation  :: best-case scenario (lower bound)
- Big \Theta (Theta) Notation :: worst and best case are the same complexity

** Constant time $O(1)$
The number of operations is always constant. An example of such computational complexity is a simple
function, which adds two numbers. Grab a, grab b, add them together and output the result = 4 steps
= constant.

#+BEGIN_SRC C
  int add_two_nums (int a, int b) {return a + b};
#+END_SRC

** Logarithmic time $O(\log{\,n})$
** Lineair time $O(n)$
Always takes n operations in the worst case scenario. The following example runs at $O(m)$, because
depending on the size of m, worst-case it will run m times.

#+BEGIN_SRC C
  for (int i = 0; i < m; i++) { /* body runs in O(1) /* }
#+END_SRC

** Linearithmic time $O(n \log{\,n})$
** Quadratic time $O(nÂ²)$
could be an example of a nested for-loop, since we have an outer loop which could run m times and
have an inner loop which could run m times. The computational complexity is $O(m^2)$.

#+BEGIN_SRC C
  for (int i = 0; i < m; i++) {
    for (int j = 0; j < m; j++) {/* body runs in O(1) /* }
  }
#+END_SRC

* Basic Algorithms [[https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html][(Visualization)]]
** Linear search
sequentially checks each element of a list untill the target value is found. Commonly used for
unsorted arrays, very poor performance on big lists. For $n$ elements in a list, it might take $n$
comparisons.
\begin{equation}O(n)\end{equation}
\begin{equation}\Omega(1)\end{equation}

** Binary search (/logarithmic search)
compares a target value to the middle element of a /sorted array/ and eliminates the other half when
it's unequal. Due to the logarithmic nature, it keeps performing well on big arrays.
\begin{equation}O(\log{\,n})\end{equation}
\begin{equation}\Omega(1)\end{equation}

** Insertion sort
is typically done in-place, by iterating up the array and leaving a sorted list behind. At each
array-position, it checks the value against the largest value of the sorted array. If larger, it
leaves the element in place. If smaller, it finds the correct index to /insert/ the value.
\begin{equation}O(n^2)\end{equation}
\begin{equation}\Omega(n)\end{equation}

** Bubble sort (/sinking sort/)
compares each pair of *adjacent pairs* and swaps them if they are in the wrong order.
\begin{equation}O(n^2)\end{equation}
\begin{equation}\Omega(n)\end{equation}

** Selection sort
is simple, but not very performant. Find the *smallest element* in the array and swap that element
with first unsorted element in the array.
\begin{equation}\Theta(n^2)\end{equation}

** Merge sort                                                    :recursion:
*** Recursion                                           :fibonacci:factorial:
In order to dive into an efficient and general-purpose sorting algorithm, we have to understand the
concept of a *recursive function*. A recursive function invokes itself as part of it's execution, also
known as the /recursive case/ of a function. A proper recursive function also has a /base case/, which
when triggered terminates the recursive process.

#+NAME: factorial.c
#+BEGIN_SRC C
  int fact (int n) {
    if (n == 1) return 1;
    else return n * fact(n-1);
  }
#+END_SRC

Effectively a recursive function can have multiple base cases, such as fibonacci, but also have
multiple recursion cases, such as the Collatz conjecture.

The *Collatz conjecture* speculates that it's always possible to end up with 1 if the following rules
are applied to a positve number $\mathbb{N}$.
- if $n = 1$ stop
- if n is even, repeat process with $n/2$
- if n is uneven, repeat process with $3n + 1$

*** Merge sort
The idea of the algorithm is to sort smaller arrays and then combine (merge) in sorted order. It
leverages recursion. For $n$ elements of a list $T(n)$ we double the amount of list, but halve the
amount of elements: $T(n) = 2T(n/2) + n$ and results in a complexity of:
\begin{equation}\Theta(n\,\log{\,n})\end{equation}

1. sort left half
2. sort right half
3. merge the two halves

* Luhn's algorithm
1. multiply every other digit by 2, starting with the 2nd-to-last digit, then add take the sum of
   the digits
2. add the sum to the sum of the other digits
3. if last digit of the total sum is 0, the card is valid
